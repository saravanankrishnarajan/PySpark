{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109c93b6",
   "metadata": {},
   "source": [
    "### Module 6: Dataframes and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf6e0c",
   "metadata": {},
   "source": [
    "#### Case Study: Mobile App Store\n",
    "#### Domain: Telecom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8592b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "A telecom software provider is building an application to monitor different telecom \n",
    "components in the production environment. For monitoring purpose, the application \n",
    "relies on log files by parsing the log files and looking for potential warning or \n",
    "exceptions in the logs and reporting them.\n",
    "\n",
    "The Dataset contains the log files from different components used in the overall \n",
    "telecom application.\n",
    "\n",
    "Tasks:\n",
    "The volume of data is quite large. As part of the R&D team, you are building a solution \n",
    "on spark to load and parse the multiple log files and then arranging the error and \n",
    "warning by the timestamp.\n",
    "1. Load data into Spark DataFrame\n",
    "2. Find out how many 404 HTTP codes are in access logs.\n",
    "3. Find out which URLs are broken.\n",
    "4. Verify there are no null columns in the original dataset.\n",
    "5. Replace null values with constants such as 0\n",
    "6. Parse timestamp to readable date.\n",
    "7. Describe which HTTP status values appear in data and how many.\n",
    "8. How many unique hosts are there in the entire log and their average request\n",
    "9. Create a spark-submit application for the same and print the findings in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b618428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0caab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import countDistinct, avg\n",
    "from pyspark.sql.functions import dayofmonth, dayofyear, year, month, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import col as func_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf4951ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cba50b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Module6CaseStudy2\").getOrCreate()\n",
    "df = df = spark.read.option(\"encoding\", \"windows-1252\").csv('651_cs2_datasets_v1.0/access.log',inferSchema=True,sep=' ')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bcdd85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = ['remote_host', '_c1', '_c2', 'request_time', 'request_zone', 'request_type', 'url', 'web_version', 'status',\n",
    " 'bytes_setnt', '_c10', 'UA1', 'UA2', 'UA3', 'UA4', 'UA5', 'UA6', 'UA7', '_c18']\n",
    "df = df.toDF(*name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "861c00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,concat_ws\n",
    "df = df.select(\"remote_host\",\n",
    "               '_c1',\n",
    "               '_c2',\n",
    "          concat_ws(' ',df['request_time'],df['request_zone']).alias('Time'),\n",
    "          df['request_type'].alias('Type'),\n",
    "         df['url'].alias('URL'),\n",
    "         df['web_version'].alias('Web'),\n",
    "         df['status'],\n",
    "         df['bytes_setnt'].alias('Bytes'),\n",
    "               '_c10',\n",
    "         concat_ws(' ','UA1','UA2','UA3','UA4','UA5','UA6','UA7').alias('UserAgent'),\n",
    "              '_c18')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6a8892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('-',None,['_c1','_c2','_c10','_c18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba7bcb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView('Access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8538097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView('Access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4dbeaac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----+--------------------+----+--------------------+--------+------+-----+--------------------+--------------------+----+\n",
      "|    remote_host| _c1| _c2|                Time|Type|                 URL|     Web|status|Bytes|                _c10|           UserAgent|_c18|\n",
      "+---------------+----+----+--------------------+----+--------------------+--------+------+-----+--------------------+--------------------+----+\n",
      "|109.169.248.247|null|null|[12/Dec/2015:18:2...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
      "|109.169.248.247|null|null|[12/Dec/2015:18:2...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
      "|    46.72.177.4|null|null|[12/Dec/2015:18:3...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
      "|    46.72.177.4|null|null|[12/Dec/2015:18:3...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
      "| 83.167.113.100|null|null|[12/Dec/2015:18:3...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
      "| 83.167.113.100|null|null|[12/Dec/2015:18:3...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
      "|   95.29.198.15|null|null|[12/Dec/2015:18:3...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
      "|   95.29.198.15|null|null|[12/Dec/2015:18:3...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
      "|  109.184.11.34|null|null|[12/Dec/2015:18:3...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
      "|  109.184.11.34|null|null|[12/Dec/2015:18:3...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
      "+---------------+----+----+--------------------+----+--------------------+--------+------+-----+--------------------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Access limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e49f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find out how many 404 HTTP codes are in access logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84900aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(status)|\n",
      "+-------------+\n",
      "|       227089|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(status) from Access where status='404'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b136146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Find out which URLs are broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "662277bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(url)|\n",
      "+----------+\n",
      "|         0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(url) from Access where length(url)=0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Verify there are no null columns in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "167df9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, remote_host: string, _c1: string, _c2: string, Time: string, Type: string, URL: string, Web: string, status: string, Bytes: string, _c10: string, UserAgent: string, _c18: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select count(isnan())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6fcfea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+----+----+---+---+------+-----+------+---------+------+\n",
      "|remote_host|    _c1|    _c2|Time|Type|URL|Web|status|Bytes|  _c10|UserAgent|  _c18|\n",
      "+-----------+-------+-------+----+----+---+---+------+-----+------+---------+------+\n",
      "|          0|2338006|2337872|   0|   0|  0| 13|     0|    0|677871|        0|796604|\n",
      "+-----------+-------+-------+----+----+---+---+------+-----+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark.sql(\"\"\"select sum(if(remote_host is Null, 1,0)) as remote_host,\n",
    "                  sum(if(_c1 is Null, 1,0)) _c1,\n",
    "                  sum(if(_c2 is Null, 1,0)) _c2,\n",
    "                  sum(if(Time is Null, 1,0)) Time,\n",
    "                  sum(if(Type is Null, 1,0)) Type,\n",
    "                  sum(if(URL is Null, 1,0)) URL,\n",
    "                  sum(if(Web is Null, 1,0)) Web,\n",
    "                  sum(if(status is Null, 1,0)) status,\n",
    "                  sum(if(Bytes is Null, 1,0)) Bytes,\n",
    "                  sum(if(_c10 is Null, 1,0)) _c10,\n",
    "                  sum(if(UserAgent is Null, 1,0)) UserAgent,\n",
    "                  sum(if(_c18 is Null, 1,0)) _c18\n",
    "          from Access\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Replace null values with constants such as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "806a6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({'_c1':0,'_c2':0, '_c10':0, '_c18':0, 'web':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1898db96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView('Access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8fd372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView('Access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36ae59a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+---+----+----+---+---+------+-----+----+---------+----+\n",
      "|remote_host|_c1|_c2|Time|Type|URL|Web|status|Bytes|_c10|UserAgent|_c18|\n",
      "+-----------+---+---+----+----+---+---+------+-----+----+---------+----+\n",
      "|          0|  0|  0|   0|   0|  0|  0|     0|    0|   0|        0|   0|\n",
      "+-----------+---+---+----+----+---+---+------+-----+----+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select sum(if(remote_host is Null, 1,0)) as remote_host,\n",
    "                  sum(if(_c1 is Null, 1,0)) _c1,\n",
    "                  sum(if(_c2 is Null, 1,0)) _c2,\n",
    "                  sum(if(Time is Null, 1,0)) Time,\n",
    "                  sum(if(Type is Null, 1,0)) Type,\n",
    "                  sum(if(URL is Null, 1,0)) URL,\n",
    "                  sum(if(Web is Null, 1,0)) Web,\n",
    "                  sum(if(status is Null, 1,0)) status,\n",
    "                  sum(if(Bytes is Null, 1,0)) Bytes,\n",
    "                  sum(if(_c10 is Null, 1,0)) _c10,\n",
    "                  sum(if(UserAgent is Null, 1,0)) UserAgent,\n",
    "                  sum(if(_c18 is Null, 1,0)) _c18\n",
    "          from Access\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Parse timestamp to readable date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e2d1a00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Date_Parsed        |\n",
      "+-------------------+\n",
      "|2015-12-12 18:25:11|\n",
      "|2015-12-12 18:25:11|\n",
      "|2015-12-12 18:31:08|\n",
      "|2015-12-12 18:31:08|\n",
      "|2015-12-12 18:31:25|\n",
      "|2015-12-12 18:31:25|\n",
      "|2015-12-12 18:32:10|\n",
      "|2015-12-12 18:32:11|\n",
      "|2015-12-12 18:32:56|\n",
      "|2015-12-12 18:32:56|\n",
      "|2015-12-12 18:33:51|\n",
      "|2015-12-12 18:33:52|\n",
      "|2015-12-12 18:36:16|\n",
      "|2015-12-12 18:36:16|\n",
      "|2015-12-12 18:38:42|\n",
      "|2015-12-12 18:38:42|\n",
      "|2015-12-12 18:38:55|\n",
      "|2015-12-12 18:38:56|\n",
      "|2015-12-12 18:39:27|\n",
      "|2015-12-12 18:39:27|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select to_timestamp(substring(Time,2,26) ,'dd/MMM/yyyy:HH:mm:ss x') Date_Parsed from Access\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Describe which HTTP status values appear in data and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0c1674f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|  status|count(status)|\n",
      "+--------+-------------+\n",
      "|     200|      1157831|\n",
      "|     206|       939929|\n",
      "|     501|          143|\n",
      "|     404|       227089|\n",
      "|     500|         3252|\n",
      "|     304|         6330|\n",
      "|     405|           83|\n",
      "|     301|          619|\n",
      "|     403|         2222|\n",
      "|     412|           19|\n",
      "|     303|          247|\n",
      "|     400|           23|\n",
      "|     401|          153|\n",
      "|HTTP/1.1|           13|\n",
      "|     406|           53|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT status,count(status) from Access group by status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83435e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. How many unique hosts are there in the entire log and their average request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "564d84b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|count(DISTINCT remote_host)|\n",
      "+---------------------------+\n",
      "|                      40836|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(remote_host)) from Access\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e3bdd90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         avg(cnt)|\n",
      "+-----------------+\n",
      "|57.25355078851993|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(cnt) from (select remote_host,count(remote_host) cnt from Access  group by remote_host) t\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dd61d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2338006|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from Access\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c899da19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.25355078851993"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2338006/40836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Create a spark-submit application for the same and print the findings in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\suzuk\\Documents\\edureka\\PySpark Certification Training Course\\module 6>spark-submit Module6_Case_Study2_Saravanan.py\n",
    "****************************************************************************************************\n",
    "1. Load data into Spark DataFrame\n",
    "****************************************************************************************************\n",
    "23/02/25 15:16:39 INFO SparkContext: Running Spark version 3.3.2\n",
    "23/02/25 15:16:39 INFO ResourceUtils: ==============================================================\n",
    "23/02/25 15:16:39 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
    "23/02/25 15:16:39 INFO ResourceUtils: ==============================================================\n",
    "23/02/25 15:16:39 INFO SparkContext: Submitted application: Module6CaseStudy2\n",
    "23/02/25 15:16:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
    "23/02/25 15:16:39 INFO ResourceProfile: Limiting resource is cpu\n",
    "23/02/25 15:16:39 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
    "23/02/25 15:16:39 INFO SecurityManager: Changing view acls to: suzuk\n",
    "23/02/25 15:16:39 INFO SecurityManager: Changing modify acls to: suzuk\n",
    "23/02/25 15:16:39 INFO SecurityManager: Changing view acls groups to:\n",
    "23/02/25 15:16:39 INFO SecurityManager: Changing modify acls groups to:\n",
    "23/02/25 15:16:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(suzuk); groups with view permissions: Set(); users  with modify permissions: Set(suzuk); groups with modify permissions: Set()\n",
    "23/02/25 15:16:40 INFO Utils: Successfully started service 'sparkDriver' on port 63716.\n",
    "23/02/25 15:16:40 INFO SparkEnv: Registering MapOutputTracker\n",
    "23/02/25 15:16:40 INFO SparkEnv: Registering BlockManagerMaster\n",
    "23/02/25 15:16:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
    "23/02/25 15:16:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
    "23/02/25 15:16:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
    "23/02/25 15:16:40 INFO DiskBlockManager: Created local directory at C:\\Users\\suzuk\\AppData\\Local\\Temp\\blockmgr-23826b14-5708-450c-a96f-e802140c34be\n",
    "23/02/25 15:16:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
    "23/02/25 15:16:40 INFO SparkEnv: Registering OutputCommitCoordinator\n",
    "23/02/25 15:16:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "23/02/25 15:16:41 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
    "23/02/25 15:16:41 INFO Executor: Starting executor ID driver on host LAPTOP-TCJS4952\n",
    "23/02/25 15:16:41 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
    "23/02/25 15:16:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63767.\n",
    "23/02/25 15:16:41 INFO NettyBlockTransferService: Server created on LAPTOP-TCJS4952:63767\n",
    "23/02/25 15:16:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
    "23/02/25 15:16:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-TCJS4952, 63767, None)\n",
    "23/02/25 15:16:41 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-TCJS4952:63767 with 366.3 MiB RAM, BlockManagerId(driver, LAPTOP-TCJS4952, 63767, None)\n",
    "23/02/25 15:16:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-TCJS4952, 63767, None)\n",
    "23/02/25 15:16:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-TCJS4952, 63767, None)\n",
    "root\n",
    " |-- _c0: string (nullable = true)\n",
    " |-- _c1: string (nullable = true)\n",
    " |-- _c2: string (nullable = true)\n",
    " |-- _c3: string (nullable = true)\n",
    " |-- _c4: string (nullable = true)\n",
    " |-- _c5: string (nullable = true)\n",
    " |-- _c6: string (nullable = true)\n",
    " |-- _c7: string (nullable = true)\n",
    " |-- _c8: string (nullable = true)\n",
    " |-- _c9: string (nullable = true)\n",
    " |-- _c10: string (nullable = true)\n",
    " |-- _c11: string (nullable = true)\n",
    " |-- _c12: string (nullable = true)\n",
    " |-- _c13: string (nullable = true)\n",
    " |-- _c14: string (nullable = true)\n",
    " |-- _c15: string (nullable = true)\n",
    " |-- _c16: string (nullable = true)\n",
    " |-- _c17: string (nullable = true)\n",
    " |-- _c18: string (nullable = true)\n",
    "\n",
    "+---------------+----+----+--------------------+----+--------------------+--------+------+-----+--------------------+--------------------+----+\n",
    "|    remote_host| _c1| _c2|                Time|Type|                 URL|     Web|status|Bytes|                _c10|           UserAgent|_c18|\n",
    "+---------------+----+----+--------------------+----+--------------------+--------+------+-----+--------------------+--------------------+----+\n",
    "|109.169.248.247|null|null|[12/Dec/2015:18:2...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
    "|109.169.248.247|null|null|[12/Dec/2015:18:2...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
    "|    46.72.177.4|null|null|[12/Dec/2015:18:3...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
    "|    46.72.177.4|null|null|[12/Dec/2015:18:3...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
    "| 83.167.113.100|null|null|[12/Dec/2015:18:3...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
    "| 83.167.113.100|null|null|[12/Dec/2015:18:3...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
    "|   95.29.198.15|null|null|[12/Dec/2015:18:3...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
    "|   95.29.198.15|null|null|[12/Dec/2015:18:3...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
    "|  109.184.11.34|null|null|[12/Dec/2015:18:3...| GET|     /administrator/|HTTP/1.1|   200| 4263|                null|Mozilla/5.0 (Wind...|null|\n",
    "|  109.184.11.34|null|null|[12/Dec/2015:18:3...|POST|/administrator/in...|HTTP/1.1|   200| 4494|http://almhuette-...|Mozilla/5.0 (Wind...|null|\n",
    "+---------------+----+----+--------------------+----+--------------------+--------+------+-----+--------------------+--------------------+----+\n",
    "\n",
    "\n",
    "****************************************************************************************************\n",
    "2. Find out how many 404 HTTP codes are in access logs.\n",
    "****************************************************************************************************\n",
    "+-------------+\n",
    "|count(status)|\n",
    "+-------------+\n",
    "|       227089|\n",
    "+-------------+\n",
    "\n",
    "\n",
    "****************************************************************************************************\n",
    "3. Find out which URLs are broken.\n",
    "****************************************************************************************************\n",
    "+----------+\n",
    "|count(url)|\n",
    "+----------+\n",
    "|         0|\n",
    "+----------+\n",
    "\n",
    "\n",
    "****************************************************************************************************\n",
    "4. Verify there are no null columns in the original dataset.\n",
    "****************************************************************************************************\n",
    "+-----------+-------+-------+----+----+---+---+------+-----+------+---------+------+\n",
    "|remote_host|    _c1|    _c2|Time|Type|URL|Web|status|Bytes|  _c10|UserAgent|  _c18|\n",
    "+-----------+-------+-------+----+----+---+---+------+-----+------+---------+------+\n",
    "|          0|2338006|2337872|   0|   0|  0| 13|     0|    0|677871|        0|796604|\n",
    "+-----------+-------+-------+----+----+---+---+------+-----+------+---------+------+\n",
    "\n",
    "\n",
    "****************************************************************************************************\n",
    "5. Replace null values with constants such as 0\n",
    "****************************************************************************************************\n",
    "+-----------+---+---+----+----+---+---+------+-----+----+---------+----+\n",
    "|remote_host|_c1|_c2|Time|Type|URL|Web|status|Bytes|_c10|UserAgent|_c18|\n",
    "+-----------+---+---+----+----+---+---+------+-----+----+---------+----+\n",
    "|          0|  0|  0|   0|   0|  0|  0|     0|    0|   0|        0|   0|\n",
    "+-----------+---+---+----+----+---+---+------+-----+----+---------+----+\n",
    "\n",
    "\n",
    "****************************************************************************************************\n",
    "6. Parse timestamp to readable date.\n",
    "****************************************************************************************************\n",
    "+-------------------+\n",
    "|Date_Parsed        |\n",
    "+-------------------+\n",
    "|2015-12-12 18:25:11|\n",
    "|2015-12-12 18:25:11|\n",
    "|2015-12-12 18:31:08|\n",
    "|2015-12-12 18:31:08|\n",
    "|2015-12-12 18:31:25|\n",
    "|2015-12-12 18:31:25|\n",
    "|2015-12-12 18:32:10|\n",
    "|2015-12-12 18:32:11|\n",
    "|2015-12-12 18:32:56|\n",
    "|2015-12-12 18:32:56|\n",
    "|2015-12-12 18:33:51|\n",
    "|2015-12-12 18:33:52|\n",
    "|2015-12-12 18:36:16|\n",
    "|2015-12-12 18:36:16|\n",
    "|2015-12-12 18:38:42|\n",
    "|2015-12-12 18:38:42|\n",
    "|2015-12-12 18:38:55|\n",
    "|2015-12-12 18:38:56|\n",
    "|2015-12-12 18:39:27|\n",
    "|2015-12-12 18:39:27|\n",
    "+-------------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "\n",
    "****************************************************************************************************\n",
    "7. Describe which HTTP status values appear in data and how many.\n",
    "****************************************************************************************************\n",
    "+--------+-------------+\n",
    "|  status|count(status)|\n",
    "+--------+-------------+\n",
    "|     200|      1157831|\n",
    "|     206|       939929|\n",
    "|     501|          143|\n",
    "|     404|       227089|\n",
    "|     500|         3252|\n",
    "|     304|         6330|\n",
    "|     405|           83|\n",
    "|     301|          619|\n",
    "|     403|         2222|\n",
    "|     412|           19|\n",
    "|     303|          247|\n",
    "|     400|           23|\n",
    "|     401|          153|\n",
    "|HTTP/1.1|           13|\n",
    "|     406|           53|\n",
    "+--------+-------------+\n",
    "\n",
    "\n",
    "****************************************************************************************************\n",
    "8. How many unique hosts are there in the entire log and their average request\n",
    "****************************************************************************************************\n",
    "+---------------------------+\n",
    "|count(DISTINCT remote_host)|\n",
    "+---------------------------+\n",
    "|                      40836|\n",
    "+---------------------------+\n",
    "\n",
    "+-----------------+\n",
    "|         avg(cnt)|\n",
    "+-----------------+\n",
    "|57.25355078851993|\n",
    "+-----------------+\n",
    "\n",
    "+--------+\n",
    "|count(1)|\n",
    "+--------+\n",
    "| 2338006|\n",
    "+--------+\n",
    "\n",
    "57.25355078851993"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
